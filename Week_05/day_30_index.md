Day 30 — Model Monitoring & Drift Detection

Run time: ~12–20 minutes per exercise (pick any 1–2 for a short session)

Overview

This day focuses on detecting and responding to model performance degradation in production.
Files in this folder are concise 15-minute practice items: quick theory + a small runnable demo or checklist.

Files created:
- `day_30_01_monitoring_fundamentals.md` — quick fundamentals and KPI checklist
- `day_30_02_data_drift_methods.py` — PSI, KS, Jensen-Shannon demos (runnable)
- `day_30_03_concept_drift_methods.py` — Page-Hinkley & simple ADWIN-like stream detector (runnable)
- `day_30_04_feature_monitoring.py` — feature-level checks (missing rate, cardinality)
- `day_30_05_prediction_monitoring.py` — prediction distribution & confidence tracking
- `day_30_06_performance_metrics.md` — metric tracking guidance and small examples
- `day_30_07_ground_truth_collection.md` — strategies for delayed labels and sampling
- `day_30_08_alerting_and_notifications.md` — design checklist for alerts
- `day_30_09_dashboards_and_viz.py` — simple visualizations (matplotlib fallback)
- `day_30_10_incident_response_and_retraining.md` — playbook checklist
- `day_30_11_exercises.md` — consolidated exercises list and capstone mini-project

Try it (quick):

Run the data drift demo:

```powershell
python .\Week_05\day_30_02_data_drift_methods.py
```

Or view the index and pick a short markdown exercise to read and apply to a local dataset.